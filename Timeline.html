# Project Arbiter — 8 Week Prototype Timeline

A single-file `timeline.md` you can paste into your GitHub repo. This file contains a compact sprint backlog, milestones, deliverables, runnable code stubs (indented code blocks so the whole file stays inside this single copy/paste block), immediate next steps, acceptance criteria, and tactical tips. Copy the entire contents of this block into `timeline.md`.

---

## Summary

**Goal:** Build an MVP Gatekeeper that performs anonymized multi‑model candidate generation, jury voting, weighted aggregation, calibration, adversarial testing, and audit logging.  
**Timeline:** **8 weeks** (focused) — adjustable to 12 weeks if you prefer a conservative pace.  
**MVP:** Gatekeeper + 3 candidate models + 3 jury models + aggregator + audit log + adversarial suite.

---

## Sprint Backlog

| Week(s) | Phase | Key Tasks | Deliverable | Est |
|---|---|---|---|---|
| 0 | Kickoff and Design | Finalize scope; confirm model access; define anonymization rules | **Architecture blueprint and Gatekeeper API contract** | 2d |
| 1 | Scaffolding | Create repo skeleton; CI; env samples; logging hooks; local mocks | **Repo skeleton, CI job, env sample** | 3d |
| 1–2 | Adapter Interfaces | Implement candidate and jury adapter interfaces; local mock adapters | **Adapter interface stubs and mock adapters** | 4d |
| 2–3 | Candidate Integration | Integrate 3 candidate models; parallel calls; rate handling; raw output store | **Working candidate adapters; raw output store** | 5d |
| 3–4 | Anonymization | Build anonymizer; fingerprint removal; formatting normalizer; tests | **Anonymizer module and tests** | 4d |
| 3–4 | Jury Integration | Integrate 3 jury models; implement vote schema; collect rationales | **Jury adapters and vote collection endpoint** | 4d |
| 4–5 | Aggregation | Implement weighted aggregator; tie rules; abstain threshold; fallback | **Aggregator module and decision API** | 4d |
| 4–5 | Calibration | Create calibration dataset; map confidences to probabilities; harness | **Calibration harness and mapping artifacts** | 3d |
| 5–6 | Sanity Filters | Implement fact checks; schema validators; hallucination heuristics | **Sanity filter module and unit tests** | 3d |
| 5–6 | Adversarial Suite | Build adversarial prompt set; batch runner; metrics capture | **Adversarial test suite and baseline metrics** | 4d |
| 6 | Logging and Audit | Implement append only event store; tamper hashes; export | **Audit log store and export script** | 3d |
| 6–7 | Performance and Cost | Add caching; adaptive sampling; cost controls; quotas | **Caching layer and sampling policy** | 3d |
| 7 | Pilot Prep | Draft runbook; human review flow; SLA draft; pilot checklist | **Runbook, pilot checklist, SLA draft** | 2d |
| 7–8 | Pilot Run | Execute pilot; collect logs; run adversarial tests; iterate | **Pilot report and metrics dashboard** | 5d |
| 8 | Handoff and Docs | Finalize README; API docs; deployment manifest; runbook | **Final docs and deployment manifest** | 2d |

---

## Milestones and Acceptance Criteria

**MVP End to End**  
- **Milestone:** Query → Candidate generation → Anonymized jury voting → Aggregation → Final answer.  
- **Acceptance:** System returns a consensus answer with provenance and confidence.

**Calibration and Safety**  
- **Milestone:** Calibration harness maps model confidences to empirical probabilities.  
- **Acceptance:** Reduced overconfidence on held‑out tests.

**Adversarial Robustness**  
- **Milestone:** Adversarial suite runs and produces baseline metrics.  
- **Acceptance:** Documented failure modes and mitigation plan.

**Auditability**  
- **Milestone:** Append only audit log with tamper evidence.  
- **Acceptance:** Every decision has stored candidate texts, votes, and final rationale.

---

## Files to Add to Repo

**Suggested file layout**  
- `timeline.md`  
- `README.md`  
- `docs/architecture.md`  
- `src/gatekeeper/adapter.py`  
- `src/gatekeeper/anonymizer.py`  
- `src/gatekeeper/aggregator.py`  
- `src/gatekeeper/gatekeeper.py`  
- `tests/adversarial/`  

---

## Code Stubs and Examples

> The code blocks below are indented so the entire markdown file remains inside this single copy/paste block. Paste the whole file into `timeline.md` and then copy each indented code block into the corresponding file in your repo when you scaffold the project.

    # src/gatekeeper/adapter.py
    from typing import Dict, Any

    class ModelAdapter:
        def send_prompt(self, prompt: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
            """Send prompt to model and return raw response dict."""
            raise NotImplementedError

        def parse_response(self, raw: Dict[str, Any]) -> Dict[str, Any]:
            """Normalize model output into canonical schema."""
            raise NotImplementedError

    # src/gatekeeper/anonymizer.py
    import re

    def anonymize_response(text: str) -> str:
        """
        Remove explicit model tags and common vendor names,
        normalize whitespace, and strip obvious fingerprints.
        """
        text = re.sub(r"

?model[:=]\s*\w+

?", "", text, flags=re.I)
        text = re.sub(r"\b(GPT-?\d+|Claude|Gemini)\b", "", text, flags=re.I)
        text = re.sub(r"\s{2,}", " ", text).strip()
        return text

    # src/gatekeeper/aggregator.py
    from collections import defaultdict
    from typing import List, Dict

    def aggregate_votes(votes: List[Dict]) -> Dict:
        """
        Simple weighted plurality aggregator.
        votes: list of {"candidate_id","vote","confidence"}
        vote: "accept" | "reject" | "abstain"
        """
        scores = defaultdict(float)
        for v in votes:
            weight = float(v.get("confidence", 0.0))
            if v.get("vote") == "accept":
                scores[v["candidate_id"]] += weight
            elif v.get("vote") == "reject":
                scores[v["candidate_id"]] -= weight * 0.5
            # abstain contributes nothing
        if not scores:
            return {"winner": None, "scores": {}}
        winner = max(scores.items(), key=lambda kv: kv[1])
        return {"winner": winner[0], "score": winner[1], "scores": dict(scores)}

    # src/gatekeeper/gatekeeper.py
    from typing import List, Dict
    from src.gatekeeper.adapter import ModelAdapter
    from src.gatekeeper.anonymizer import anonymize_response
    from src.gatekeeper.aggregator import aggregate_votes

    def gatekeeper_query(prompt: str, candidate_adapters: List[ModelAdapter], jury_adapters: List[ModelAdapter]) -> Dict:
        """
        Minimal end-to-end flow:
        1) Generate candidates from candidate_adapters
        2) Anonymize candidates and present to jury_adapters
        3) Collect votes and aggregate
        """
        # 1. Generate candidates
        candidates = []
        for i, adapter in enumerate(candidate_adapters):
            raw = adapter.send_prompt(prompt, {"role": "candidate", "id": f"c{i}"})
            parsed = adapter.parse_response(raw)
            candidates.append({"id": f"c{i}", "text": parsed.get("text", "")})

        # 2. Anonymize and present to juries
        votes = []
        for j, jury in enumerate(jury_adapters):
            for c in candidates:
                anon = anonymize_response(c["text"])
                raw_vote = jury.send_prompt(anon, {"role": "jury", "candidate_id": c["id"]})
                parsed_vote = jury.parse_response(raw_vote)
                votes.append(parsed_vote)

        # 3. Aggregate
        result = aggregate_votes(votes)
        return {"result": result, "candidates": candidates, "votes": votes}

---

## Immediate Next Steps

1. **Create repo** and add this file as `timeline.md`.  
2. **Add README** with a short project overview and quick start.  
3. **Add docs/architecture.md** with a system diagram and Gatekeeper API contract.  
4. **Provision model API keys** or set up local mocks for development.  
5. **Add a simple CI job** that runs linters and unit tests on push.  
6. **Start with local mocks** to iterate quickly and avoid API costs.

---

## Acceptance Criteria and Tactical Tips

**Acceptance criteria**  
- System returns a consensus answer with provenance and confidence.  
- Calibration harness reduces overconfidence on held out tests.  
- Adversarial suite produces baseline metrics and documented mitigations.  
- Audit logs store candidate texts, votes, and final rationale.

**Tactical tips**  
- Return a provisional answer fast and finalize asynchronously if full consensus takes longer.  
- Use transparent abstention when consensus confidence is low to build trust.  
- Cache frequent queries and use adaptive sampling to control cost.  
- Keep anonymization rules aggressive to avoid fingerprint leakage.  
- Start with three candidate models and three jury models for the MVP and iterate weighting rules after adversarial testing.

---

## Notes

- This file is intentionally self-contained so you can paste it directly into `timeline.md`.  
- The indented code blocks are ready to be copied into the corresponding files under `src/gatekeeper/`.  
- If you want, I can now generate a single-block `README.md` and `docs/architecture.md` in the same format for easy copy/paste.